# General
test_mode: True
debug: False
run_num: 0
explicit_run_enumeration: False
machine: "local"
device: "cuda"
workdir: "/mnt/d/Aptamer RL/ActiveLearningPipeline/Runs"
# Seeds
seeds:
  sampler: 0
  model: 0
  dataset: 0
  toy_oracle: 0
# Dataset
dataset:
  oracle: "linear"
  type: "toy"
  init_length: 100
  dict_size: 4
  variable_length: True
  min_length: 10
  max_length: 40
  sample_tasks: 1
# AL
al:
  sample_method: "mcmc"
  query_mode: "learned"
  n_iter: 10
  episodes: 1000
  query_selection: "clustering"
  minima_dist_cutoff: 0.25
  queries_per_iter: 100
  mode: "test_rl"
  q_batch_size: 8
  buffer_size: 10000
  hyperparams_learning: True
  energy_uncertainty_tradeoff: 0
  action_state_size: 9
  q_network_width: 1000
  comet:
    project: aptamer-al-mk
    tags:
      - EX_test_run
# Querier
querier: 
  model_state_size: 30
  opt: "SGD"
  momentum: 0.95
  model_ckpt: !!null
  latent_space_width: 9
# GFlowNet
gflownet:
  device: "cpu"
  model_ckpt: !!null #model.pt
  progress: 1
  opt: "adam"
  adam_beta1: 0.9
  adam_beta2: 0.999
  momentum: 0.9
  mbsize: 64
  train_to_sample_ratio: 1
  n_hid: 256
  n_layers: 2
  n_iter: 200
  n_samples: 10000
  num_empirical_loss: 200000
  batch_reward: 1
  bootstrap_tau: 0.0
  clip_grad_norm: 0.0
  annealing: True
  post_annealing_samples: 100
  post_annealing_time: 1000
  min_word_len: 1
  max_word_len: 1
  comet:
    project: aptamer-al-mk
    tags:
      - testing
  early_stopping: 0.1
  ema_alpha: 0.5
  learning_rate: 0.0001
  ckpt_period: 100
  reward_beta_init: 1
  reward_beta_mult: 1.25
  reward_beta_period: 100
  reward_max: 10000
# Proxy model
proxy:
  model_type: "mlp"
  training_parallelism: False
  ensemble_size: 10
  width: 256
  embedding_dim: 256
  n_layers: 2
  mbsize: 10
  max_epochs: 200
  shuffle_dataset: True
# MCMC
mcmc:
  sampling_time: 200
  num_samplers: 20
  stun_min_gamma: -3
  stun_max_gamma: 1

